{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Operations on word vectors\n \nBecause word embeddings are very computionally expensive to train, most ML practitioners will load a pre-trained set of embeddings. \n\n**Steps:**\n\n- Load pre-trained word vectors\n- Measure similarity using cosine similarity\n- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.  \n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": " Downloading....... Glove vector from http://nlp.stanford.edu/data/glove.6B.zip", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "--2018-07-22 10:12:37--  http://nlp.stanford.edu/data/glove.6B.zip\nResolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://nlp.stanford.edu/data/glove.6B.zip [following]\n--2018-07-22 10:12:38--  https://nlp.stanford.edu/data/glove.6B.zip\nConnecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 862182613 (822M) [application/zip]\nSaving to: \u2018glove.6B.zip\u2019\n\n100%[======================================>] 862,182,613 27.2MB/s   in 28s    \n\n2018-07-22 10:13:06 (29.4 MB/s) - \u2018glove.6B.zip\u2019 saved [862182613/862182613]\n\n"
                }
            ], 
            "source": "!wget http://nlp.stanford.edu/data/glove.6B.zip"
        }, 
        {
            "source": "Exracting file........ ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from zipfile import ZipFile\nwith ZipFile('glove.6B.zip', 'r') as zf:\n    zf.extractall('')"
        }, 
        {
            "source": "You can use any of the following glove vector :", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\r\nglove.6B.200d.txt  glove.6B.50d.txt\r\n"
                }
            ], 
            "source": "!ls"
        }, 
        {
            "source": "Defining a set of 'words' from glove vector and their mapping into a dictionary called 'word_to_vec_map'.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import numpy as np "
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def read_glove_vecs(glove_file):\n    with open(glove_file, 'r') as f:\n        words = set()\n        word_to_vec_map = {}\n        \n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n            \n    return words, word_to_vec_map"
        }, 
        {
            "source": "Next, lets load the word vectors. For this assignment, we will use 50-dimensional GloVe vectors to represent words. Run the following cell to load the `word_to_vec_map`. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
        }, 
        {
            "source": "\n- `words`: set of words in the vocabulary.\n- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation.\n\nYou've seen that one-hot vectors do not do a good job cpaturing what words are similar. GloVe vectors provide much more useful information about the meaning of individual words. Lets now see how you can use GloVe vectors to decide how similar two words are. \n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Cosine similarity\n\nTo measure how similar two words are, we need a way to measure the degree of similarity between two embedding vectors for the two words. Given two vectors $u$ and $v$, cosine similarity is defined as follows: \n\n    CosineSimilarity(u, v) = {u . v} / {||u||_2 ||v||_2} = cos(theta)\u00a0\n\nwhere {u.v} is the dot product (or inner product) of two vectors,  '||u||_2'  is the norm (or length) of the vector 'u' , and 'theta' is the angle between 'u' and 'v'. This similarity depends on the angle between 'u' and 'v'. If 'u' and 'v' are very similar, their cosine similarity will be close to 1; if they are dissimilar, the cosine similarity will take a smaller value. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def cosine_similarity(u, v):\n    \"\"\"\n    Cosine similarity reflects the degree of similariy between u and v\n        \n    Arguments:\n        u -- a word vector of shape (n,)          \n        v -- a word vector of shape (n,)\n\n    Returns:\n        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n    \"\"\"\n    \n    distance = 0.0\n    \n    \n    # Compute the dot product between u and v\n    dot = np.dot(u,v)\n    # Compute the L2 norm of u (\u22481 line)\n    norm_u = np.sqrt(np.sum(u * u))\n    \n    # Compute the L2 norm of v (\u22481 line)\n    norm_v = np.sqrt(np.sum(v * v))\n    # Compute the cosine similarity \n    cosine_similarity = dot/(norm_u*norm_v)\n    \n    \n    return cosine_similarity"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "cosine_similarity(father, mother) =  0.890903844289\ncosine_similarity(ball, crocodile) =  0.274392462614\ncosine_similarity(france - paris, rome - italy) =  -0.675147930817\n"
                }
            ], 
            "source": "father = word_to_vec_map[\"father\"]\nmother = word_to_vec_map[\"mother\"]\nball = word_to_vec_map[\"ball\"]\ncrocodile = word_to_vec_map[\"crocodile\"]\nfrance = word_to_vec_map[\"france\"]\nitaly = word_to_vec_map[\"italy\"]\nparis = word_to_vec_map[\"paris\"]\nrome = word_to_vec_map[\"rome\"]\n\nprint(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\nprint(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\nprint(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
        }, 
        {
            "source": "## Word analogy task\n\nIn the word analogy task, we complete the sentence <font color='brown'>\"*a* is to *b* as *c* is to **____**\"</font>. An example is <font color='brown'> '*man* is to *woman* as *king* is to *queen*' </font>. In detail, we are trying to find a word *d*, such that the associated word vectors $e_a, e_b, e_c, e_d$ are related in the following manner: $e_b - e_a \\approx e_d - e_c$. We will measure the similarity between $e_b - e_a$ and $e_d - e_c$ using cosine similarity. \n\n**Exercise**: Complete the code below to be able to perform word analogies!", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n    \"\"\"\n    Performs the word analogy task as explained above: a is to b as c is to ____. \n    \n    Arguments:\n    word_a -- a word, string\n    word_b -- a word, string\n    word_c -- a word, string\n    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n    \n    Returns:\n    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n    \"\"\"\n    \n    # convert words to lower case\n    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n    \n    \n    # Get the word embeddings v_a, v_b and v_c (\u22481-3 lines)\n    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]\n    \n    \n    words = word_to_vec_map.keys()\n    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n\n    # loop over the whole word vector set\n    for w in words:        \n        # to avoid best_word being one of the input words, pass on them.\n        if w in [word_a, word_b, word_c] :\n            continue\n        \n        \n        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (\u22481 line)\n        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n        \n        # If the cosine_sim is more than the max_cosine_sim seen so far,\n            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (\u22483 lines)\n        if cosine_sim > max_cosine_sim:\n            max_cosine_sim = cosine_sim\n            best_word = w\n        \n        \n    return best_word"
        }, 
        {
            "source": "Run the cell below to test your code, this may take 1-2 minutes.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "italy -> italian :: spain -> spanish\nindia -> delhi :: china -> beijing\nman -> woman :: boy -> girl\nsmall -> smaller :: large -> larger\n"
                }
            ], 
            "source": "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'china'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\nfor triad in triads_to_try:\n    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "coursera": {
            "course_slug": "nlp-sequence-models", 
            "launcher_item_id": "5NrJ6", 
            "graded_item_id": "8hb5s"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}
